{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<ul> <li> <p> High-Stakes Ready</p> <p>Built for mission-critical applications where errors carry catastrophic consequences</p> <p> Learn more</p> </li> <li> <p> Dual Explainer Support</p> <p>Integrates both SHAP and LIME for comprehensive explainability</p> <p> Explanation Types</p> </li> <li> <p> Real-Time &amp; Auditing</p> <p>Support for both operational oversight and post-mortem analysis</p> <p> Getting Started</p> </li> <li> <p> Easy Integration</p> <p>Simple API that works with any scikit-learn compatible model</p> <p> API Reference</p> </li> </ul>"},{"location":"#black-box-precision-core-sdk","title":"Black Box Precision Core SDK","text":"<p>Unlocking High-Stakes Performance with Explainable AI</p> <p> Get Started  Documentation</p>"},{"location":"#overview","title":"Overview","text":"<p>The Black Box Precision SDK resolves the dilemma between AI performance and interpretability. It enables you to harness maximum AI power while simultaneously integrating Explainable Artificial Intelligence (XAI) techniques to ensure transparency, safety, and accountability\u2014without sacrificing performance.</p> <p>This SDK is specifically designed for high-stakes environments where errors carry catastrophic consequences:</p> <ul> <li>\ud83c\udfe5 Medical Diagnostics - Clinical trust and regulatory compliance</li> <li>\ud83d\ude97 Autonomous Systems - Safety verification and post-incident analysis  </li> <li>\ud83d\udcb0 Financial Systems - Regulatory compliance and bias detection</li> <li>\ud83d\udee1\ufe0f Military Applications - Mission-critical decision validation</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#shap-integration","title":"\ud83d\udd2c SHAP Integration","text":"<p>Theoretical gold standard for feature attribution with mathematical guarantees. Ideal for post-mortem auditing and regulatory compliance.</p>"},{"location":"#lime-integration","title":"\u26a1 LIME Integration","text":"<p>Fast, intuitive local explanations perfect for real-time operational oversight and split-second decision validation.</p>"},{"location":"#global-local-explanations","title":"\ud83c\udf10 Global &amp; Local Explanations","text":"<p>Support for both auditing (global) and operational oversight (local) modes.</p>"},{"location":"#high-stakes-ready","title":"\ud83d\udee1\ufe0f High-Stakes Ready","text":"<p>Built specifically for mission-critical applications where transparency is non-negotiable.</p>"},{"location":"#comprehensive-utilities","title":"\ud83d\udcca Comprehensive Utilities","text":"<p>Tools for validation, aggregation, and audit trails to support regulatory compliance.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\n# Train a black box model\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Initialize Black Box Precision framework\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=[f\"feature_{i}\" for i in range(10)]\n)\n\n# Generate explanation for operational oversight\nX_test = np.random.rand(1, 10)\nresult = bbp.explain_local(X_test)\n\nprint(\"Prediction:\", result[\"predictions\"])\nprint(\"SHAP Explanation:\", result[\"explanations\"][\"shap\"])\nprint(\"LIME Explanation:\", result[\"explanations\"][\"lime\"])\n</code></pre>"},{"location":"#installation","title":"Installation","text":"npmpip <pre><code>npm install blackboxpcs\n</code></pre> <p>\ud83d\udce6 npm package: https://www.npmjs.com/package/blackboxpcs</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Or install as a package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Black Box Precision embraces the full complexity of deep AI, viewing the \"Black Box\" as a source of unparalleled power, not a failure of design. Our approach is built on three non-negotiable pillars:</p> <ol> <li>Depth of Insight: Utilize complex models to their full capacity</li> <li>Trust through Results: Generate verifiable explanations for every decision</li> <li>Application in Critical Fields: Designed for high-stakes environments</li> </ol>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quick Start Tutorial - Get up and running in minutes</li> <li>API Reference - Complete API documentation</li> <li>Examples - Real-world usage examples</li> <li>Use Cases - Industry-specific applications</li> </ul> <p>The time to choose is now: Demand Black Box Precision.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for Black Box Precision Core SDK.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"<p>The Black Box Precision SDK provides a unified interface for explainable AI through the following main components:</p> <ul> <li>BlackBoxPrecision - Main framework class</li> <li>SHAPExplainer - SHAP-based explanations</li> <li>LIMEExplainer - LIME-based explanations</li> <li>Utilities - Helper functions</li> </ul>"},{"location":"api-reference/#quick-reference","title":"Quick Reference","text":""},{"location":"api-reference/#core-classes","title":"Core Classes","text":"<pre><code>from blackboxpcs import (\n    BlackBoxPrecision,\n    SHAPExplainer,\n    LIMEExplainer,\n    ExplanationType,\n    ExplanationMode\n)\n</code></pre>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":"<pre><code>from blackboxpcs.utils import (\n    validate_explanation,\n    aggregate_explanations,\n    format_explanation_for_audit,\n    compare_explanations,\n    extract_key_features\n)\n</code></pre>"},{"location":"api-reference/#enums","title":"Enums","text":""},{"location":"api-reference/#explanationtype","title":"ExplanationType","text":"<pre><code>class ExplanationType(Enum):\n    SHAP = \"shap\"    # SHAP explanations only\n    LIME = \"lime\"    # LIME explanations only\n    BOTH = \"both\"    # Both SHAP and LIME\n</code></pre>"},{"location":"api-reference/#explanationmode","title":"ExplanationMode","text":"<pre><code>class ExplanationMode(Enum):\n    GLOBAL = \"global\"  # For auditing and development\n    LOCAL = \"local\"    # For operational oversight\n</code></pre>"},{"location":"api-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/#basic-usage","title":"Basic Usage","text":"<pre><code>bbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=feature_names\n)\n\nresult = bbp.explain_local(X)\n</code></pre>"},{"location":"api-reference/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Custom explainer initialization\nshap_explainer = SHAPExplainer(\n    model=model,\n    background_data=X_train[:100],\n    algorithm=\"kernel\"\n)\n\nresult = shap_explainer.explain(X, mode=ExplanationMode.GLOBAL)\n</code></pre>"},{"location":"api-reference/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>BlackBoxPrecision - Main framework API</li> <li>SHAPExplainer - SHAP explainer API</li> <li>LIMEExplainer - LIME explainer API</li> <li>Utilities - Utility functions API</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Black Box Precision Core SDK!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Clone your fork:    <pre><code>git clone https://github.com/your-username/blackbox-core-sdk.git\ncd blackbox-core-sdk\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre> <p>With coverage:</p> <pre><code>pytest --cov=blackboxpcs --cov-report=html\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use <code>black</code> for formatting and <code>flake8</code> for linting:</p> <pre><code>black blackboxpcs/\nflake8 blackboxpcs/\n</code></pre>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"contributing/#code-style_1","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use type hints where appropriate</li> <li>Write docstrings for all public functions and classes</li> <li>Keep functions focused and single-purpose</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for any API changes</li> <li>Add examples for new features</li> <li>Keep docstrings up to date</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new features</li> <li>Ensure all tests pass</li> <li>Aim for high test coverage</li> </ul>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Create a feature branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes</li> <li>Write/update tests</li> <li>Update documentation</li> <li>Run tests and linting</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":"<ul> <li>Additional explainer implementations</li> <li>Performance optimizations</li> <li>Documentation improvements</li> <li>Example use cases</li> <li>Bug fixes</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub for questions or discussions.</p> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>Understanding the fundamental concepts behind Black Box Precision.</p>"},{"location":"core-concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Black Box Precision provides a unified framework for integrating Explainable AI (XAI) techniques with black box machine learning models. The framework is designed around three core principles:</p> <ol> <li>Model Agnostic: Works with any model that supports <code>predict</code> or <code>predict_proba</code></li> <li>Dual Explainer Support: Integrates both SHAP and LIME for comprehensive explanations</li> <li>Mode Flexibility: Supports both operational (local) and auditing (global) use cases</li> </ol>"},{"location":"core-concepts/#explanation-types","title":"Explanation Types","text":""},{"location":"core-concepts/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"<p>SHAP provides the theoretical gold standard for feature attribution, calculating the fair marginal contribution of each input feature to the prediction.</p> <p>Characteristics: - Mathematical guarantees (efficiency, symmetry, dummy, additivity) - Model-agnostic and consistent - Computationally intensive for large datasets</p> <p>Best For: - Post-mortem auditing - Regulatory compliance - Understanding feature importance - Detecting bias</p>"},{"location":"core-concepts/#lime-local-interpretable-model-agnostic-explanations","title":"LIME (Local Interpretable Model-agnostic Explanations)","text":"<p>LIME provides fast, intuitive explanations by training a simple, local surrogate model around a single prediction point.</p> <p>Characteristics: - Fast computation - Intuitive explanations - Local approximations - Less computationally intensive</p> <p>Best For: - Real-time operational oversight - Split-second decision validation - Quick feature identification - Interactive debugging</p>"},{"location":"core-concepts/#explanation-modes","title":"Explanation Modes","text":""},{"location":"core-concepts/#local-mode-operational","title":"Local Mode (Operational)","text":"<p>Local explanations focus on individual predictions, providing insights into why a specific decision was made.</p> <p>Use Cases: - Real-time decision validation - Operational oversight - Interactive debugging - User-facing explanations</p> <p>Example: <pre><code>result = bbp.explain_local(X[0:1])\n# Explains why this specific instance received this prediction\n</code></pre></p>"},{"location":"core-concepts/#global-mode-auditing","title":"Global Mode (Auditing)","text":"<p>Global explanations analyze model behavior across datasets, detecting systemic patterns and biases.</p> <p>Use Cases: - Model auditing - Bias detection - Feature importance analysis - Regulatory compliance</p> <p>Example: <pre><code>audit_result = bbp.explain_global(X[:100])\n# Analyzes model behavior across multiple instances\n</code></pre></p>"},{"location":"core-concepts/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"core-concepts/#operational-workflow","title":"Operational Workflow","text":"<pre><code># 1. Initialize framework\nbbp = BlackBoxPrecision(model, explainer_type=ExplanationType.LIME)\n\n# 2. Generate real-time explanation\nresult = bbp.predict_with_explanation(new_instance)\n\n# 3. Extract key features\ntop_features = extract_key_features(result, top_k=5)\n\n# 4. Use for decision validation\nif top_features[\"features\"][0][\"importance\"] &gt; threshold:\n    approve_decision()\n</code></pre>"},{"location":"core-concepts/#auditing-workflow","title":"Auditing Workflow","text":"<pre><code># 1. Initialize framework\nbbp = BlackBoxPrecision(model, explainer_type=ExplanationType.SHAP)\n\n# 2. Perform comprehensive audit\naudit_result = bbp.audit_model(X_train, y_train)\n\n# 3. Analyze feature importance\nfeature_importance = audit_result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"]\n\n# 4. Check for bias\nif suspicious_feature in top_features:\n    flag_for_review()\n</code></pre>"},{"location":"core-concepts/#model-compatibility","title":"Model Compatibility","text":"<p>Black Box Precision works with any model that implements:</p> <ul> <li><code>predict(X)</code> - For classification or regression</li> <li><code>predict_proba(X)</code> - For classification (preferred)</li> </ul> <p>Compatible Models: - scikit-learn models (RandomForest, XGBoost, SVM, etc.) - TensorFlow/Keras models - PyTorch models (with wrapper) - Custom models (if they implement predict methods)</p>"},{"location":"core-concepts/#feature-names-and-class-names","title":"Feature Names and Class Names","text":"<p>Providing feature names and class names enhances the quality of explanations:</p> <pre><code>bbp = BlackBoxPrecision(\n    model=model,\n    feature_names=[\"age\", \"income\", \"credit_score\"],\n    class_names=[\"denied\", \"approved\"]\n)\n</code></pre> <p>Benefits: - More readable explanations - Better visualization - Easier debugging - Regulatory compliance</p>"},{"location":"core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Explanation Types - Deep dive into SHAP and LIME</li> <li>API Reference - Complete API documentation</li> <li>Examples - Practical usage examples</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Real-world examples demonstrating Black Box Precision capabilities.</p>"},{"location":"examples/#basic-classification","title":"Basic Classification","text":"<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\n# Generate data\nX, y = make_classification(n_samples=200, n_features=10, random_state=42)\nfeature_names = [f\"feature_{i}\" for i in range(10)]\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X, y)\n\n# Initialize framework\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=feature_names,\n    class_names=[\"negative\", \"positive\"]\n)\n\n# Generate explanation\nresult = bbp.predict_with_explanation(X[0:1])\n\nprint(\"Prediction:\", result[\"predictions\"])\nprint(\"SHAP Values:\", result[\"explanations\"][\"shap\"][\"shap_values\"])\nprint(\"LIME Weights:\", result[\"explanations\"][\"lime\"][\"explanations\"][\"feature_weights\"])\n</code></pre>"},{"location":"examples/#model-auditing","title":"Model Auditing","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.SHAP,\n    feature_names=feature_names\n)\n\n# Comprehensive audit\naudit_result = bbp.audit_model(\n    X_train[:100],\n    y=y_train[:100]\n)\n\nprint(\"Model Accuracy:\", audit_result.get(\"accuracy\"))\nprint(\"\\nFeature Importance Ranking:\")\nfor feature, importance in audit_result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"][:10]:\n    print(f\"  {feature}: {importance:.4f}\")\n</code></pre>"},{"location":"examples/#batch-explanations","title":"Batch Explanations","text":"<pre><code># Explain multiple instances\nresults = []\nfor instance in X_test[:10]:\n    result = bbp.explain_local(instance.reshape(1, -1))\n    results.append(result)\n\n# Aggregate explanations\nfrom blackboxpcs.utils import aggregate_explanations\n\naggregated = aggregate_explanations(results, method=\"mean\")\nprint(\"Aggregated SHAP Values:\", aggregated[\"shap_values\"])\n</code></pre>"},{"location":"examples/#feature-extraction","title":"Feature Extraction","text":"<pre><code>from blackboxpcs.utils import extract_key_features\n\nresult = bbp.explain_local(X_test[0:1])\n\n# Extract top features from SHAP\ntop_shap = extract_key_features(result, top_k=5, explainer_type=\"shap\")\nprint(\"Top SHAP Features:\")\nfor feature in top_shap[\"features\"]:\n    print(f\"  {feature.get('name', feature['index'])}: {feature['importance']:.4f}\")\n\n# Extract top features from LIME\ntop_lime = extract_key_features(result, top_k=5, explainer_type=\"lime\")\nprint(\"\\nTop LIME Features:\")\nfor feature in top_lime[\"features\"]:\n    print(f\"  {feature['name']}: {feature['importance']:.4f}\")\n</code></pre>"},{"location":"examples/#explanation-validation","title":"Explanation Validation","text":"<pre><code>from blackboxpcs.utils import validate_explanation, compare_explanations\n\n# Validate single explanation\nresult = bbp.explain_local(X_test[0:1])\nvalidation = validate_explanation(result)\n\nif validation[\"is_valid\"]:\n    print(\"\u2713 Explanation is valid\")\nelse:\n    print(\"\u2717 Validation issues:\", validation)\n\n# Compare two explanations\nresult1 = bbp.explain_local(X_test[0:1])\nresult2 = bbp.explain_local(X_test[1:2])\n\ncomparison = compare_explanations(result1, result2, metric=\"cosine\")\nprint(\"SHAP Similarity:\", comparison.get(\"shap_similarity\"))\nprint(\"Prediction Match:\", comparison.get(\"prediction_match\"))\n</code></pre>"},{"location":"examples/#audit-trail-generation","title":"Audit Trail Generation","text":"<pre><code>from blackboxpcs.utils import format_explanation_for_audit\nimport json\nfrom datetime import datetime\n\nresult = bbp.explain_local(X_test[0:1])\naudit_record = format_explanation_for_audit(result, include_raw=False)\naudit_record[\"timestamp\"] = datetime.now().isoformat()\n\n# Save to audit log\nwith open(\"audit_log.json\", \"a\") as f:\n    json.dump(audit_record, f)\n    f.write(\"\\n\")\n</code></pre>"},{"location":"examples/#custom-model-integration","title":"Custom Model Integration","text":"<pre><code>import numpy as np\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\nclass CustomModel:\n    def __init__(self):\n        self.weights = np.random.randn(10)\n\n    def predict(self, X):\n        return (X @ self.weights &gt; 0).astype(int)\n\n    def predict_proba(self, X):\n        probs = 1 / (1 + np.exp(-X @ self.weights))\n        return np.column_stack([1 - probs, probs])\n\n# Use with custom model\ncustom_model = CustomModel()\nbbp = BlackBoxPrecision(\n    model=custom_model,\n    explainer_type=ExplanationType.BOTH\n)\n\nresult = bbp.explain_local(X_test[0:1])\n</code></pre>"},{"location":"examples/#regression-example","title":"Regression Example","text":"<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\n# Generate regression data\nX, y = make_regression(n_samples=200, n_features=10, random_state=42)\n\n# Train regression model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X, y)\n\n# Initialize with LIME (better for regression)\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.LIME,\n    feature_names=[f\"feature_{i}\" for i in range(10)]\n)\n\n# Explain regression prediction\nresult = bbp.explain_local(X[0:1].reshape(1, -1))\nprint(\"Prediction:\", result[\"predictions\"])\nprint(\"Feature Contributions:\", result[\"explanations\"][\"lime\"][\"explanations\"][\"feature_weights\"])\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Use Cases - Industry-specific examples</li> <li>API Reference - Complete API documentation</li> <li>Contributing - Add your own examples</li> </ul>"},{"location":"explanation-types/","title":"Explanation Types","text":"<p>Deep dive into SHAP and LIME explainers.</p>"},{"location":"explanation-types/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"<p>SHAP provides mathematically grounded feature attribution based on cooperative game theory.</p>"},{"location":"explanation-types/#theory","title":"Theory","text":"<p>SHAP values satisfy four important properties:</p> <ol> <li>Efficiency: Sum of SHAP values equals prediction minus baseline</li> <li>Symmetry: Features with equal contributions get equal SHAP values</li> <li>Dummy: Features that don't affect output get zero SHAP values</li> <li>Additivity: SHAP values are additive across features</li> </ol>"},{"location":"explanation-types/#algorithms","title":"Algorithms","text":"<p>Black Box Precision automatically selects the best SHAP algorithm:</p> <ul> <li>Tree: For tree-based models (RandomForest, XGBoost, etc.)</li> <li>Kernel: For any model with background data</li> <li>Permutation: For any model without background data</li> <li>Exact: For small datasets (computationally expensive)</li> </ul>"},{"location":"explanation-types/#usage","title":"Usage","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.SHAP,\n    background_data=X_train[:100],  # Recommended for better explanations\n    algorithm=\"auto\"  # Auto-selects best algorithm\n)\n\nresult = bbp.explain_local(X_test[0:1])\nshap_values = result[\"explanations\"][\"shap\"][\"shap_values\"]\n</code></pre>"},{"location":"explanation-types/#advanced-usage","title":"Advanced Usage","text":"<pre><code>from blackboxpcs.explainers import SHAPExplainer\n\nshap_explainer = SHAPExplainer(\n    model=model,\n    feature_names=feature_names,\n    background_data=X_train[:100],\n    algorithm=\"kernel\"\n)\n\n# Get feature attribution for specific feature\nattribution = shap_explainer.get_feature_attribution(\n    X_test[0:1],\n    feature_idx=0,\n    class_idx=1\n)\n</code></pre>"},{"location":"explanation-types/#lime-local-interpretable-model-agnostic-explanations","title":"LIME (Local Interpretable Model-agnostic Explanations)","text":"<p>LIME explains individual predictions by training a local surrogate model.</p>"},{"location":"explanation-types/#theory_1","title":"Theory","text":"<p>LIME works by:</p> <ol> <li>Sampling instances around the prediction point</li> <li>Training a simple, interpretable model on these samples</li> <li>Using the simple model's coefficients as feature importance</li> </ol>"},{"location":"explanation-types/#characteristics","title":"Characteristics","text":"<ul> <li>Fast: Typically faster than SHAP</li> <li>Local: Explains individual predictions, not global behavior</li> <li>Intuitive: Easy to understand explanations</li> <li>Flexible: Works with tabular, text, and image data</li> </ul>"},{"location":"explanation-types/#usage_1","title":"Usage","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.LIME,\n    num_features=10  # Number of top features to show\n)\n\nresult = bbp.explain_local(X_test[0:1])\nlime_explanation = result[\"explanations\"][\"lime\"][\"explanations\"]\n</code></pre>"},{"location":"explanation-types/#advanced-usage_1","title":"Advanced Usage","text":"<pre><code>from blackboxpcs.explainers import LIMEExplainer\n\nlime_explainer = LIMEExplainer(\n    model=model,\n    feature_names=feature_names,\n    mode=\"classification\",\n    num_features=10\n)\n\n# Get top contributing features\ntop_features = lime_explainer.get_top_features(\n    X_test[0:1],\n    top_k=5\n)\n</code></pre>"},{"location":"explanation-types/#choosing-between-shap-and-lime","title":"Choosing Between SHAP and LIME","text":""},{"location":"explanation-types/#use-shap-when","title":"Use SHAP When:","text":"<ul> <li>\u2705 You need mathematical guarantees</li> <li>\u2705 Regulatory compliance is required</li> <li>\u2705 You're doing post-mortem analysis</li> <li>\u2705 You have time for computation</li> <li>\u2705 You need global feature importance</li> </ul>"},{"location":"explanation-types/#use-lime-when","title":"Use LIME When:","text":"<ul> <li>\u2705 You need real-time explanations</li> <li>\u2705 Speed is critical</li> <li>\u2705 You're debugging individual predictions</li> <li>\u2705 You want intuitive explanations</li> <li>\u2705 You're doing operational oversight</li> </ul>"},{"location":"explanation-types/#use-both-when","title":"Use Both When:","text":"<ul> <li>\u2705 You need comprehensive coverage</li> <li>\u2705 You want to validate explanations</li> <li>\u2705 You're building production systems</li> <li>\u2705 You need both speed and rigor</li> </ul>"},{"location":"explanation-types/#comparison-example","title":"Comparison Example","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH\n)\n\nresult = bbp.explain_local(X_test[0:1])\n\n# Compare SHAP and LIME explanations\nshap_features = result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"][:5]\nlime_features = sorted(\n    result[\"explanations\"][\"lime\"][\"explanations\"][\"feature_weights\"].items(),\n    key=lambda x: abs(x[1]),\n    reverse=True\n)[:5]\n\nprint(\"Top 5 Features (SHAP):\", shap_features)\nprint(\"Top 5 Features (LIME):\", lime_features)\n</code></pre>"},{"location":"explanation-types/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Examples - See SHAP and LIME in action</li> <li>Use Cases - Industry-specific applications</li> </ul>"},{"location":"getting-started/","title":"Quick Start Guide","text":"<p>Get up and running with Black Box Precision in minutes.</p>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#step-1-train-your-model","title":"Step 1: Train Your Model","text":"<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\n# Generate or load your data\nX, y = make_classification(\n    n_samples=200,\n    n_features=10,\n    n_informative=5,\n    random_state=42\n)\n\n# Train a black box model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X, y)\n</code></pre>"},{"location":"getting-started/#step-2-initialize-black-box-precision","title":"Step 2: Initialize Black Box Precision","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,  # Use both SHAP and LIME\n    feature_names=[f\"feature_{i}\" for i in range(10)],\n    class_names=[\"class_0\", \"class_1\"]\n)\n</code></pre>"},{"location":"getting-started/#step-3-generate-explanations","title":"Step 3: Generate Explanations","text":"<pre><code># Generate local explanation for a single prediction\ntest_instance = X[0:1]\nresult = bbp.explain_local(test_instance)\n\nprint(\"Prediction:\", result[\"predictions\"])\nprint(\"SHAP Explanation:\", result[\"explanations\"][\"shap\"])\nprint(\"LIME Explanation:\", result[\"explanations\"][\"lime\"])\n</code></pre>"},{"location":"getting-started/#explanation-types","title":"Explanation Types","text":""},{"location":"getting-started/#shap-only","title":"SHAP Only","text":"<pre><code>bbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.SHAP,\n    feature_names=feature_names\n)\n\nresult = bbp.explain_local(X[0:1])\nshap_values = result[\"explanations\"][\"shap\"][\"shap_values\"]\n</code></pre>"},{"location":"getting-started/#lime-only","title":"LIME Only","text":"<pre><code>bbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.LIME,\n    feature_names=feature_names\n)\n\nresult = bbp.explain_local(X[0:1])\nlime_explanation = result[\"explanations\"][\"lime\"][\"explanations\"]\n</code></pre>"},{"location":"getting-started/#explanation-modes","title":"Explanation Modes","text":""},{"location":"getting-started/#local-explanations-operational","title":"Local Explanations (Operational)","text":"<p>For real-time decision validation:</p> <pre><code># Single prediction with explanation\nresult = bbp.explain_local(X[0:1])\n\n# Or use the convenience method\nresult = bbp.predict_with_explanation(X[0:1])\n</code></pre>"},{"location":"getting-started/#global-explanations-auditing","title":"Global Explanations (Auditing)","text":"<p>For model auditing and bias detection:</p> <pre><code># Analyze model behavior across dataset\naudit_result = bbp.explain_global(X[:100])\n\n# Comprehensive audit with accuracy\naudit_result = bbp.audit_model(\n    X[:100],\n    y=y[:100],\n    explanation_type=ExplanationType.SHAP\n)\n\nprint(\"Model Accuracy:\", audit_result.get(\"accuracy\"))\nprint(\"Feature Importance:\", audit_result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"])\n</code></pre>"},{"location":"getting-started/#working-with-results","title":"Working with Results","text":""},{"location":"getting-started/#extract-key-features","title":"Extract Key Features","text":"<pre><code>from blackboxpcs.utils import extract_key_features\n\nresult = bbp.explain_local(X[0:1])\ntop_features = extract_key_features(result, top_k=5, explainer_type=\"shap\")\n\nfor feature in top_features[\"features\"]:\n    print(f\"{feature['name']}: {feature['importance']:.4f}\")\n</code></pre>"},{"location":"getting-started/#validate-explanations","title":"Validate Explanations","text":"<pre><code>from blackboxpcs.utils import validate_explanation\n\nresult = bbp.explain_local(X[0:1])\nvalidation = validate_explanation(result)\n\nif validation[\"is_valid\"]:\n    print(\"Explanation is valid!\")\nelse:\n    print(\"Validation issues:\", validation)\n</code></pre>"},{"location":"getting-started/#format-for-audit-trail","title":"Format for Audit Trail","text":"<pre><code>from blackboxpcs.utils import format_explanation_for_audit\n\nresult = bbp.explain_local(X[0:1])\naudit_record = format_explanation_for_audit(result, include_raw=False)\n\n# Save to audit log\nimport json\nwith open(\"audit_log.json\", \"a\") as f:\n    json.dump(audit_record, f)\n</code></pre>"},{"location":"getting-started/#complete-example","title":"Complete Example","text":"<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\nfrom blackboxpcs.utils import extract_key_features\n\n# 1. Prepare data\nX, y = make_classification(n_samples=200, n_features=10, random_state=42)\nfeature_names = [f\"feature_{i}\" for i in range(10)]\n\n# 2. Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X, y)\n\n# 3. Initialize framework\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=feature_names,\n    class_names=[\"negative\", \"positive\"]\n)\n\n# 4. Generate explanation\nresult = bbp.predict_with_explanation(X[0:1])\n\n# 5. Extract insights\ntop_features = extract_key_features(result, top_k=3)\n\nprint(\"Prediction:\", result[\"predictions\"])\nprint(\"\\nTop Contributing Features:\")\nfor feature in top_features[\"features\"]:\n    print(f\"  {feature.get('name', feature['index'])}: {feature['importance']:.4f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Understand the framework architecture</li> <li>API Reference - Explore all available methods</li> <li>Examples - See advanced usage patterns</li> <li>Use Cases - Industry-specific applications</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Install Black Box Precision Core SDK using your preferred method.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>NumPy &gt;= 1.21.0</li> <li>scikit-learn &gt;= 1.0.0</li> <li>SHAP &gt;= 0.41.0</li> <li>LIME &gt;= 0.2.0.1</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#via-npm","title":"Via npm","text":"<p>The package is available on npm:</p> <pre><code>npm install blackboxpcs\n</code></pre> <p>\ud83d\udce6 npm package: https://www.npmjs.com/package/blackboxpcs</p> <p>After installation via npm, you'll need to install Python dependencies:</p> <pre><code>pip install numpy scikit-learn shap lime\n</code></pre>"},{"location":"installation/#via-pip-python","title":"Via pip (Python)","text":""},{"location":"installation/#install-from-requirementstxt","title":"Install from requirements.txt","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#install-as-a-package","title":"Install as a package","text":"<pre><code>pip install -e .\n</code></pre> <p>This installs the package in editable mode, allowing you to make changes to the source code.</p>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/blackbox-pcs/blackbox-core-sdk.git\ncd blackbox-core-sdk\n</code></pre> <ol> <li>Install dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Install the package:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#development-dependencies","title":"Development Dependencies","text":"<p>For development and testing:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs: - pytest &gt;= 7.0.0 - pytest-cov &gt;= 4.0.0 - black &gt;= 22.0.0 - flake8 &gt;= 5.0.0</p>"},{"location":"installation/#visualization-dependencies","title":"Visualization Dependencies","text":"<p>For visualization capabilities:</p> <pre><code>pip install -e \".[visualization]\"\n</code></pre> <p>This installs: - matplotlib &gt;= 3.5.0 - seaborn &gt;= 0.12.0</p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import blackboxpcs\nprint(blackboxpcs.__version__)\n\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors:</p> <ol> <li> <p>Ensure all dependencies are installed:    <pre><code>pip install numpy scikit-learn shap lime\n</code></pre></p> </li> <li> <p>Verify Python version:    <pre><code>python --version  # Should be 3.8+\n</code></pre></p> </li> </ol>"},{"location":"installation/#shap-installation-issues","title":"SHAP Installation Issues","text":"<p>If SHAP fails to install:</p> <pre><code>pip install --upgrade pip\npip install shap\n</code></pre>"},{"location":"installation/#lime-installation-issues","title":"LIME Installation Issues","text":"<p>If LIME fails to install:</p> <pre><code>pip install lime\n</code></pre> <p>For Windows users, you may need Visual C++ Build Tools.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with your first explanation</li> <li>API Reference - Explore the complete API</li> <li>Examples - See real-world usage examples</li> </ul>"},{"location":"api/blackboxprecision/","title":"BlackBoxPrecision","text":"<p>Main framework class for integrating XAI with black box models.</p>"},{"location":"api/blackboxprecision/#class-definition","title":"Class Definition","text":"<pre><code>class BlackBoxPrecision:\n    def __init__(\n        self,\n        model: Any,\n        explainer_type: ExplanationType = ExplanationType.BOTH,\n        feature_names: Optional[List[str]] = None,\n        class_names: Optional[List[str]] = None,\n        **kwargs\n    ):\n        ...\n</code></pre>"},{"location":"api/blackboxprecision/#parameters","title":"Parameters","text":""},{"location":"api/blackboxprecision/#model","title":"<code>model</code>","text":"<p>The black box model to explain. Must support <code>predict()</code> or <code>predict_proba()</code> methods.</p> <p>Type: Any Required: Yes</p>"},{"location":"api/blackboxprecision/#explainer_type","title":"<code>explainer_type</code>","text":"<p>Type of explainer(s) to use.</p> <p>Type: <code>ExplanationType</code> Default: <code>ExplanationType.BOTH</code> Options: - <code>ExplanationType.SHAP</code> - SHAP only - <code>ExplanationType.LIME</code> - LIME only - <code>ExplanationType.BOTH</code> - Both SHAP and LIME</p>"},{"location":"api/blackboxprecision/#feature_names","title":"<code>feature_names</code>","text":"<p>Optional list of feature names for better interpretability.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/blackboxprecision/#class_names","title":"<code>class_names</code>","text":"<p>Optional list of class names for classification tasks.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/blackboxprecision/#kwargs","title":"<code>**kwargs</code>","text":"<p>Additional arguments passed to explainers (e.g., <code>background_data</code>, <code>algorithm</code>, <code>num_features</code>).</p>"},{"location":"api/blackboxprecision/#methods","title":"Methods","text":""},{"location":"api/blackboxprecision/#explain","title":"<code>explain()</code>","text":"<p>Generate explanations for predictions.</p> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    mode: ExplanationMode = ExplanationMode.LOCAL,\n    explanation_type: Optional[ExplanationType] = None,\n    X_background: Optional[np.ndarray] = None,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data to explain (single instance or batch) - <code>mode</code>: Explanation mode (<code>GLOBAL</code> for auditing, <code>LOCAL</code> for operational) - <code>explanation_type</code>: Override default explainer type - <code>X_background</code>: Background data for SHAP (optional) - <code>**kwargs</code>: Additional arguments for explainers</p> <p>Returns: Dictionary containing explanations and metadata</p> <p>Example: <pre><code>result = bbp.explain(X_test, mode=ExplanationMode.GLOBAL)\n</code></pre></p>"},{"location":"api/blackboxprecision/#explain_local","title":"<code>explain_local()</code>","text":"<p>Generate local explanations for operational oversight.</p> <pre><code>def explain_local(\n    self,\n    X: np.ndarray,\n    explanation_type: Optional[ExplanationType] = None,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data to explain - <code>explanation_type</code>: Override default explainer type - <code>**kwargs</code>: Additional arguments for explainers</p> <p>Returns: Dictionary containing local explanations</p> <p>Example: <pre><code>result = bbp.explain_local(X_test[0:1])\n</code></pre></p>"},{"location":"api/blackboxprecision/#explain_global","title":"<code>explain_global()</code>","text":"<p>Generate global explanations for auditing and development.</p> <pre><code>def explain_global(\n    self,\n    X: np.ndarray,\n    explanation_type: Optional[ExplanationType] = None,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data to explain (typically training/validation set) - <code>explanation_type</code>: Override default explainer type - <code>**kwargs</code>: Additional arguments for explainers</p> <p>Returns: Dictionary containing global explanations</p> <p>Example: <pre><code>result = bbp.explain_global(X_train[:100])\n</code></pre></p>"},{"location":"api/blackboxprecision/#predict_with_explanation","title":"<code>predict_with_explanation()</code>","text":"<p>Make predictions with immediate explanations.</p> <pre><code>def predict_with_explanation(\n    self,\n    X: np.ndarray,\n    explanation_type: Optional[ExplanationType] = None,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data - <code>explanation_type</code>: Override default explainer type - <code>**kwargs</code>: Additional arguments for explainers</p> <p>Returns: Dictionary with predictions and explanations</p> <p>Example: <pre><code>result = bbp.predict_with_explanation(X_test[0:1])\n</code></pre></p>"},{"location":"api/blackboxprecision/#audit_model","title":"<code>audit_model()</code>","text":"<p>Perform comprehensive model auditing using global XAI.</p> <pre><code>def audit_model(\n    self,\n    X: np.ndarray,\n    y: Optional[np.ndarray] = None,\n    explanation_type: Optional[ExplanationType] = None,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Dataset for auditing (typically training/validation set) - <code>y</code>: Optional ground truth labels for validation - <code>explanation_type</code>: Override default explainer type - <code>**kwargs</code>: Additional arguments for explainers</p> <p>Returns: Dictionary containing audit results and global explanations</p> <p>Example: <pre><code>audit_result = bbp.audit_model(X_train, y_train)\nprint(\"Accuracy:\", audit_result.get(\"accuracy\"))\n</code></pre></p>"},{"location":"api/blackboxprecision/#return-format","title":"Return Format","text":"<p>All explanation methods return a dictionary with the following structure:</p> <pre><code>{\n    \"predictions\": np.ndarray,  # Model predictions\n    \"mode\": str,                 # \"local\" or \"global\"\n    \"explanations\": {\n        \"shap\": {                # If SHAP is enabled\n            \"shap_values\": np.ndarray,\n            \"base_values\": np.ndarray,\n            \"algorithm\": str,\n            \"feature_importance\": List[float],  # Global mode only\n            \"feature_importance_ranking\": List[Tuple[str, float]]  # Global mode only\n        },\n        \"lime\": {                # If LIME is enabled\n            \"explanations\": Dict,\n            \"feature_importance\": Dict,  # Global mode only\n            \"feature_importance_ranking\": List[Tuple[str, float]]  # Global mode only\n        }\n    }\n}\n</code></pre>"},{"location":"api/blackboxprecision/#examples","title":"Examples","text":""},{"location":"api/blackboxprecision/#basic-usage","title":"Basic Usage","text":"<pre><code>from blackboxpcs import BlackBoxPrecision, ExplanationType\n\nbbp = BlackBoxPrecision(\n    model=model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=[\"age\", \"income\", \"credit_score\"]\n)\n\nresult = bbp.explain_local(X_test[0:1])\n</code></pre>"},{"location":"api/blackboxprecision/#model-auditing","title":"Model Auditing","text":"<pre><code>audit_result = bbp.audit_model(\n    X_train[:100],\n    y=y_train[:100],\n    explanation_type=ExplanationType.SHAP\n)\n\nprint(\"Model Accuracy:\", audit_result.get(\"accuracy\"))\nprint(\"Top Features:\", audit_result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"][:5])\n</code></pre>"},{"location":"api/blackboxprecision/#custom-background-data","title":"Custom Background Data","text":"<pre><code>result = bbp.explain(\n    X_test[0:1],\n    X_background=X_train[:100],  # Custom background for SHAP\n    explanation_type=ExplanationType.SHAP\n)\n</code></pre>"},{"location":"api/limeexplainer/","title":"LIMEExplainer","text":"<p>LIME (Local Interpretable Model-agnostic Explanations) Explainer for fast local explanations.</p>"},{"location":"api/limeexplainer/#class-definition","title":"Class Definition","text":"<pre><code>class LIMEExplainer(BaseExplainer):\n    def __init__(\n        self,\n        model: Any,\n        feature_names: Optional[List[str]] = None,\n        class_names: Optional[List[str]] = None,\n        mode: str = \"classification\",\n        num_features: int = 10,\n        **kwargs\n    ):\n        ...\n</code></pre>"},{"location":"api/limeexplainer/#parameters","title":"Parameters","text":""},{"location":"api/limeexplainer/#model","title":"<code>model</code>","text":"<p>The black box model to explain.</p> <p>Type: Any Required: Yes</p>"},{"location":"api/limeexplainer/#feature_names","title":"<code>feature_names</code>","text":"<p>Optional feature names for better interpretability.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/limeexplainer/#class_names","title":"<code>class_names</code>","text":"<p>Optional class names for classification tasks.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/limeexplainer/#mode","title":"<code>mode</code>","text":"<p>Task mode.</p> <p>Type: <code>str</code> Default: <code>\"classification\"</code> Options: - <code>\"classification\"</code> - For classification tasks - <code>\"regression\"</code> - For regression tasks</p>"},{"location":"api/limeexplainer/#num_features","title":"<code>num_features</code>","text":"<p>Number of top features to show in explanation.</p> <p>Type: <code>int</code> Default: <code>10</code></p>"},{"location":"api/limeexplainer/#methods","title":"Methods","text":""},{"location":"api/limeexplainer/#explain","title":"<code>explain()</code>","text":"<p>Generate LIME explanations.</p> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    mode: ExplanationMode = ExplanationMode.LOCAL,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data to explain - <code>mode</code>: Explanation mode (<code>GLOBAL</code> or <code>LOCAL</code>) - <code>**kwargs</code>: Additional LIME parameters (e.g., <code>num_features</code>)</p> <p>Returns: Dictionary containing LIME explanations</p> <p>Example: <pre><code>from blackboxpcs.explainers import LIMEExplainer\n\nlime_explainer = LIMEExplainer(\n    model=model,\n    num_features=10,\n    mode=\"classification\"\n)\n\nresult = lime_explainer.explain(X_test[0:1])\n</code></pre></p>"},{"location":"api/limeexplainer/#get_top_features","title":"<code>get_top_features()</code>","text":"<p>Get top contributing features for the prediction.</p> <pre><code>def get_top_features(\n    self,\n    X: np.ndarray,\n    top_k: Optional[int] = None\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data (single instance) - <code>top_k</code>: Number of top features to return (None for all)</p> <p>Returns: Dictionary with top features and their contributions</p> <p>Example: <pre><code>top_features = lime_explainer.get_top_features(X_test[0:1], top_k=5)\n\nfor feature_name, weight in top_features[\"top_features\"]:\n    print(f\"{feature_name}: {weight:.4f}\")\n</code></pre></p>"},{"location":"api/limeexplainer/#return-format","title":"Return Format","text":"<pre><code>{\n    \"explanations\": Dict,  # Single instance or list for batch\n    \"mode\": str,\n    \"num_features\": int,\n    \"feature_names\": List[str],\n    \"class_names\": List[str],\n    \"feature_importance\": Dict,  # Global mode only\n    \"feature_importance_ranking\": List[Tuple[str, float]]  # Global mode only\n}\n</code></pre>"},{"location":"api/limeexplainer/#examples","title":"Examples","text":""},{"location":"api/limeexplainer/#basic-usage","title":"Basic Usage","text":"<pre><code>from blackboxpcs.explainers import LIMEExplainer\n\nlime_explainer = LIMEExplainer(\n    model=model,\n    feature_names=[\"age\", \"income\", \"credit_score\"],\n    num_features=10\n)\n\nresult = lime_explainer.explain(X_test[0:1])\nfeature_weights = result[\"explanations\"][\"feature_weights\"]\n</code></pre>"},{"location":"api/limeexplainer/#get-top-features","title":"Get Top Features","text":"<pre><code>top_features = lime_explainer.get_top_features(X_test[0:1], top_k=5)\n\nprint(\"Top Contributing Features:\")\nfor feature_name, weight in top_features[\"top_features\"]:\n    print(f\"  {feature_name}: {weight:.4f}\")\n</code></pre>"},{"location":"api/limeexplainer/#regression-mode","title":"Regression Mode","text":"<pre><code>lime_explainer = LIMEExplainer(\n    model=regression_model,\n    mode=\"regression\",\n    num_features=10\n)\n\nresult = lime_explainer.explain(X_test[0:1])\n</code></pre>"},{"location":"api/shapexplainer/","title":"SHAPExplainer","text":"<p>SHAP (SHapley Additive exPlanations) Explainer for feature attribution.</p>"},{"location":"api/shapexplainer/#class-definition","title":"Class Definition","text":"<pre><code>class SHAPExplainer(BaseExplainer):\n    def __init__(\n        self,\n        model: Any,\n        feature_names: Optional[List[str]] = None,\n        class_names: Optional[List[str]] = None,\n        background_data: Optional[np.ndarray] = None,\n        algorithm: str = \"auto\",\n        **kwargs\n    ):\n        ...\n</code></pre>"},{"location":"api/shapexplainer/#parameters","title":"Parameters","text":""},{"location":"api/shapexplainer/#model","title":"<code>model</code>","text":"<p>The black box model to explain.</p> <p>Type: Any Required: Yes</p>"},{"location":"api/shapexplainer/#feature_names","title":"<code>feature_names</code>","text":"<p>Optional feature names for better interpretability.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/shapexplainer/#class_names","title":"<code>class_names</code>","text":"<p>Optional class names for classification tasks.</p> <p>Type: <code>Optional[List[str]]</code> Default: <code>None</code></p>"},{"location":"api/shapexplainer/#background_data","title":"<code>background_data</code>","text":"<p>Background dataset for SHAP (recommended for better explanations).</p> <p>Type: <code>Optional[np.ndarray]</code> Default: <code>None</code></p>"},{"location":"api/shapexplainer/#algorithm","title":"<code>algorithm</code>","text":"<p>SHAP algorithm to use.</p> <p>Type: <code>str</code> Default: <code>\"auto\"</code> Options: - <code>\"auto\"</code> - Auto-selects best algorithm - <code>\"tree\"</code> - For tree-based models - <code>\"kernel\"</code> - For any model with background data - <code>\"permutation\"</code> - For any model without background data - <code>\"exact\"</code> - Exact SHAP (computationally expensive) - <code>\"sampling\"</code> - Sampling-based approximation</p>"},{"location":"api/shapexplainer/#methods","title":"Methods","text":""},{"location":"api/shapexplainer/#explain","title":"<code>explain()</code>","text":"<p>Generate SHAP explanations.</p> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    mode: ExplanationMode = ExplanationMode.LOCAL,\n    **kwargs\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data to explain - <code>mode</code>: Explanation mode (<code>GLOBAL</code> or <code>LOCAL</code>) - <code>**kwargs</code>: Additional SHAP parameters</p> <p>Returns: Dictionary containing SHAP values and metadata</p> <p>Example: <pre><code>from blackboxpcs.explainers import SHAPExplainer\n\nshap_explainer = SHAPExplainer(\n    model=model,\n    background_data=X_train[:100],\n    algorithm=\"kernel\"\n)\n\nresult = shap_explainer.explain(X_test[0:1])\nshap_values = result[\"shap_values\"]\n</code></pre></p>"},{"location":"api/shapexplainer/#get_feature_attribution","title":"<code>get_feature_attribution()</code>","text":"<p>Get feature attribution for specific features/classes.</p> <pre><code>def get_feature_attribution(\n    self,\n    X: np.ndarray,\n    feature_idx: Optional[int] = None,\n    class_idx: Optional[int] = None\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>X</code>: Input data - <code>feature_idx</code>: Specific feature index (None for all) - <code>class_idx</code>: Specific class index for classification (None for all)</p> <p>Returns: Dictionary with feature attributions</p> <p>Example: <pre><code>attribution = shap_explainer.get_feature_attribution(\n    X_test[0:1],\n    feature_idx=0,\n    class_idx=1\n)\n</code></pre></p>"},{"location":"api/shapexplainer/#return-format","title":"Return Format","text":"<pre><code>{\n    \"shap_values\": np.ndarray,\n    \"base_values\": np.ndarray,\n    \"mode\": str,\n    \"algorithm\": str,\n    \"feature_names\": List[str],\n    \"class_names\": List[str],\n    \"feature_importance\": List[float],  # Global mode only\n    \"feature_importance_ranking\": List[Tuple[str, float]]  # Global mode only\n}\n</code></pre>"},{"location":"api/shapexplainer/#examples","title":"Examples","text":""},{"location":"api/shapexplainer/#basic-usage","title":"Basic Usage","text":"<pre><code>from blackboxpcs.explainers import SHAPExplainer\n\nshap_explainer = SHAPExplainer(\n    model=model,\n    feature_names=[\"age\", \"income\", \"credit_score\"],\n    background_data=X_train[:100]\n)\n\nresult = shap_explainer.explain(X_test[0:1])\n</code></pre>"},{"location":"api/shapexplainer/#global-feature-importance","title":"Global Feature Importance","text":"<pre><code>result = shap_explainer.explain(\n    X_train[:100],\n    mode=ExplanationMode.GLOBAL\n)\n\nprint(\"Feature Importance Ranking:\")\nfor feature, importance in result[\"feature_importance_ranking\"][:10]:\n    print(f\"  {feature}: {importance:.4f}\")\n</code></pre>"},{"location":"api/shapexplainer/#specific-feature-attribution","title":"Specific Feature Attribution","text":"<pre><code>attribution = shap_explainer.get_feature_attribution(\n    X_test[0:1],\n    feature_idx=0  # First feature\n)\n\nprint(\"Attribution:\", attribution[\"attributions\"])\n</code></pre>"},{"location":"api/utils/","title":"Utility Functions","text":"<p>Helper functions for validation, aggregation, and workflow management.</p>"},{"location":"api/utils/#functions","title":"Functions","text":""},{"location":"api/utils/#validate_explanation","title":"<code>validate_explanation()</code>","text":"<p>Validate an explanation for completeness and consistency.</p> <pre><code>def validate_explanation(\n    explanation: Dict[str, Any],\n    prediction: Optional[np.ndarray] = None\n) -&gt; Dict[str, bool]:\n</code></pre> <p>Parameters: - <code>explanation</code>: Explanation dictionary from explainer - <code>prediction</code>: Optional prediction to validate against</p> <p>Returns: Dictionary with validation results</p> <p>Example: <pre><code>from blackboxpcs.utils import validate_explanation\n\nresult = bbp.explain_local(X_test[0:1])\nvalidation = validate_explanation(result)\n\nif validation[\"is_valid\"]:\n    print(\"\u2713 Explanation is valid\")\n</code></pre></p>"},{"location":"api/utils/#aggregate_explanations","title":"<code>aggregate_explanations()</code>","text":"<p>Aggregate multiple explanations for global analysis.</p> <pre><code>def aggregate_explanations(\n    explanations: List[Dict[str, Any]],\n    method: str = \"mean\"\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>explanations</code>: List of explanation dictionaries - <code>method</code>: Aggregation method (<code>\"mean\"</code>, <code>\"median\"</code>, <code>\"max\"</code>, <code>\"min\"</code>)</p> <p>Returns: Aggregated explanation dictionary</p> <p>Example: <pre><code>from blackboxpcs.utils import aggregate_explanations\n\nresults = [bbp.explain_local(X[i:i+1]) for i in range(10)]\naggregated = aggregate_explanations(results, method=\"mean\")\n</code></pre></p>"},{"location":"api/utils/#format_explanation_for_audit","title":"<code>format_explanation_for_audit()</code>","text":"<p>Format explanation for audit trail and regulatory compliance.</p> <pre><code>def format_explanation_for_audit(\n    explanation: Dict[str, Any],\n    include_raw: bool = False\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>explanation</code>: Explanation dictionary - <code>include_raw</code>: Whether to include raw explanation data</p> <p>Returns: Formatted explanation suitable for audit logs</p> <p>Example: <pre><code>from blackboxpcs.utils import format_explanation_for_audit\nimport json\n\nresult = bbp.explain_local(X_test[0:1])\naudit_record = format_explanation_for_audit(result, include_raw=False)\n\nwith open(\"audit_log.json\", \"a\") as f:\n    json.dump(audit_record, f)\n</code></pre></p>"},{"location":"api/utils/#compare_explanations","title":"<code>compare_explanations()</code>","text":"<p>Compare two explanations to measure consistency.</p> <pre><code>def compare_explanations(\n    explanation1: Dict[str, Any],\n    explanation2: Dict[str, Any],\n    metric: str = \"cosine\"\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>explanation1</code>: First explanation - <code>explanation2</code>: Second explanation - <code>metric</code>: Comparison metric (<code>\"cosine\"</code>, <code>\"euclidean\"</code>, <code>\"manhattan\"</code>)</p> <p>Returns: Dictionary with comparison results</p> <p>Example: <pre><code>from blackboxpcs.utils import compare_explanations\n\nresult1 = bbp.explain_local(X_test[0:1])\nresult2 = bbp.explain_local(X_test[1:2])\n\ncomparison = compare_explanations(result1, result2, metric=\"cosine\")\nprint(\"Similarity:\", comparison.get(\"shap_similarity\"))\n</code></pre></p>"},{"location":"api/utils/#extract_key_features","title":"<code>extract_key_features()</code>","text":"<p>Extract top contributing features from an explanation.</p> <pre><code>def extract_key_features(\n    explanation: Dict[str, Any],\n    top_k: int = 5,\n    explainer_type: Optional[str] = None\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Parameters: - <code>explanation</code>: Explanation dictionary - <code>top_k</code>: Number of top features to extract - <code>explainer_type</code>: Specific explainer to use (<code>\"shap\"</code> or <code>\"lime\"</code>)</p> <p>Returns: Dictionary with top features and their contributions</p> <p>Example: <pre><code>from blackboxpcs.utils import extract_key_features\n\nresult = bbp.explain_local(X_test[0:1])\ntop_features = extract_key_features(result, top_k=5, explainer_type=\"shap\")\n\nfor feature in top_features[\"features\"]:\n    print(f\"{feature.get('name', feature['index'])}: {feature['importance']:.4f}\")\n</code></pre></p>"},{"location":"api/utils/#examples","title":"Examples","text":""},{"location":"api/utils/#complete-workflow","title":"Complete Workflow","text":"<pre><code>from blackboxpcs.utils import (\n    validate_explanation,\n    extract_key_features,\n    format_explanation_for_audit\n)\n\n# Generate explanation\nresult = bbp.explain_local(X_test[0:1])\n\n# Validate\nvalidation = validate_explanation(result)\nassert validation[\"is_valid\"], \"Invalid explanation\"\n\n# Extract key features\ntop_features = extract_key_features(result, top_k=5)\n\n# Format for audit\naudit_record = format_explanation_for_audit(result)\n</code></pre>"},{"location":"use-cases/autonomous/","title":"Autonomous Systems Use Case","text":"<p>Using Black Box Precision for real-time decision validation in autonomous systems.</p>"},{"location":"use-cases/autonomous/#challenge","title":"Challenge","text":"<p>Validating safety-critical, split-second decisions in autonomous systems requires:</p> <ul> <li>Real-time explanation generation</li> <li>Fast computation for operational oversight</li> <li>Post-incident analysis capabilities</li> <li>Safety verification</li> </ul>"},{"location":"use-cases/autonomous/#solution","title":"Solution","text":"<p>Black Box Precision with LIME provides instant explanations for real-time validation, while SHAP enables comprehensive post-incident analysis.</p>"},{"location":"use-cases/autonomous/#implementation","title":"Implementation","text":""},{"location":"use-cases/autonomous/#setup","title":"Setup","text":"<pre><code>import numpy as np\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\n# Autonomous vehicle perception model\nperception_model = load_perception_model()\n\n# Initialize with LIME for real-time explanations\nbbp = BlackBoxPrecision(\n    model=perception_model,\n    explainer_type=ExplanationType.LIME,\n    feature_names=[f\"sensor_{i}\" for i in range(100)],  # Sensor readings\n    num_features=10  # Top 10 features for quick understanding\n)\n</code></pre>"},{"location":"use-cases/autonomous/#real-time-decision-validation","title":"Real-Time Decision Validation","text":"<pre><code># Sensor data at decision point\nsensor_data = np.array([...])  # Real-time sensor reading\n\n# Real-time explanation for critical decision\nresult = bbp.explain_local(sensor_data)\n\n# Get top contributing features quickly\nfrom blackboxpcs.explainers import LIMEExplainer\nlime_explainer = bbp._get_lime_explainer()\ntop_features = lime_explainer.get_top_features(sensor_data, top_k=10)\n\nprint(f\"Decision: {result['predictions']}\")\nprint(\"\\nKey Factors:\")\nfor feature_name, weight in top_features[\"top_features\"]:\n    print(f\"  {feature_name}: {weight:.4f}\")\n\n# Validate decision\nif top_features[\"top_features\"][0][1] &gt; threshold:\n    execute_action()\nelse:\n    request_human_intervention()\n</code></pre>"},{"location":"use-cases/autonomous/#post-incident-analysis","title":"Post-Incident Analysis","text":"<pre><code># After incident, use SHAP for comprehensive analysis\nbbp_shap = BlackBoxPrecision(\n    model=perception_model,\n    explainer_type=ExplanationType.SHAP,\n    background_data=historical_sensor_data\n)\n\n# Analyze incident data\nincident_data = load_incident_data()\nresult = bbp_shap.explain_global(incident_data)\n\n# Extract detailed feature importance\nfeature_importance = result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"]\n\nprint(\"Post-Incident Analysis:\")\nfor feature, importance in feature_importance[:10]:\n    print(f\"  {feature}: {importance:.4f}\")\n</code></pre>"},{"location":"use-cases/autonomous/#batch-processing-for-monitoring","title":"Batch Processing for Monitoring","text":"<pre><code># Monitor multiple decision points\ndecision_points = load_recent_decisions()\n\nresults = []\nfor point in decision_points:\n    result = bbp.explain_local(point)\n    results.append(result)\n\n# Aggregate for pattern detection\nfrom blackboxpcs.utils import aggregate_explanations\n\naggregated = aggregate_explanations(results, method=\"mean\")\n\n# Detect anomalies\nif detect_anomaly(aggregated):\n    trigger_safety_review()\n</code></pre>"},{"location":"use-cases/autonomous/#impact","title":"Impact","text":"<ul> <li>\u2705 Safety Verification: Validate critical decisions in real-time</li> <li>\u2705 Compliance: Post-incident analysis for regulatory bodies</li> <li>\u2705 Debugging: Understand model behavior for system improvement</li> <li>\u2705 Trust: Build confidence in autonomous systems</li> </ul>"},{"location":"use-cases/autonomous/#best-practices","title":"Best Practices","text":"<ol> <li>Use LIME for Real-Time: Fast enough for operational oversight</li> <li>Use SHAP for Analysis: Comprehensive post-incident investigation</li> <li>Monitor Patterns: Aggregate explanations to detect anomalies</li> <li>Maintain Logs: Keep explanations for safety audits</li> </ol>"},{"location":"use-cases/autonomous/#related","title":"Related","text":"<ul> <li>Medical Diagnostics - Regulatory compliance</li> <li>Financial Systems - Risk assessment</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"use-cases/financial/","title":"Financial Systems Use Case","text":"<p>Using Black Box Precision for credit decisions and fraud detection.</p>"},{"location":"use-cases/financial/#challenge","title":"Challenge","text":"<p>Explaining credit decisions and fraud detection requires:</p> <ul> <li>Regulatory compliance (e.g., Fair Lending Act)</li> <li>Customer trust and transparency</li> <li>Bias detection</li> <li>Audit trails</li> </ul>"},{"location":"use-cases/financial/#solution","title":"Solution","text":"<p>Black Box Precision with combined SHAP and LIME provides comprehensive explanations for financial decisions, enabling regulatory compliance and customer trust.</p>"},{"location":"use-cases/financial/#implementation","title":"Implementation","text":""},{"location":"use-cases/financial/#credit-decision-explanation","title":"Credit Decision Explanation","text":"<pre><code>import numpy as np\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\n# Credit scoring model\ncredit_model = load_credit_model()\n\n# Initialize with both explainers\nbbp = BlackBoxPrecision(\n    model=credit_model,\n    explainer_type=ExplanationType.BOTH,\n    feature_names=[\n        \"credit_score\",\n        \"income\",\n        \"debt_to_income\",\n        \"employment_years\",\n        \"loan_amount\",\n        \"loan_term\"\n    ],\n    class_names=[\"denied\", \"approved\"]\n)\n</code></pre>"},{"location":"use-cases/financial/#credit-decision-with-explanation","title":"Credit Decision with Explanation","text":"<pre><code># Applicant data\napplicant_data = np.array([[\n    720,   # credit_score\n    75000, # income\n    0.35,  # debt_to_income\n    5,     # employment_years\n    25000, # loan_amount\n    60     # loan_term (months)\n]])\n\n# Get decision with explanation\nresult = bbp.predict_with_explanation(applicant_data)\n\n# Extract key factors\nfrom blackboxpcs.utils import extract_key_features\n\ntop_features = extract_key_features(result, top_k=5)\n\nprint(f\"Decision: {result['predictions']}\")\nprint(\"\\nKey Factors:\")\nfor feature in top_features[\"features\"]:\n    print(f\"  {feature.get('name', feature['index'])}: {feature['importance']:.4f}\")\n\n# Generate customer-facing explanation\ncustomer_explanation = format_customer_explanation(result, top_features)\nsend_to_customer(customer_explanation)\n</code></pre>"},{"location":"use-cases/financial/#bias-detection","title":"Bias Detection","text":"<pre><code># Audit model for bias\naudit_result = bbp.audit_model(\n    X_train,\n    y=y_train,\n    explanation_type=ExplanationType.SHAP\n)\n\n# Check for protected class bias\nprotected_features = [\"age\", \"gender\", \"race\", \"zip_code\"]\nfeature_importance = audit_result[\"explanations\"][\"shap\"][\"feature_importance_ranking\"]\n\nprint(\"Bias Analysis:\")\nfor feature, importance in feature_importance:\n    if feature in protected_features:\n        if importance &gt; bias_threshold:\n            print(f\"\u26a0\ufe0f  WARNING: High importance for protected feature {feature}\")\n            flag_for_review()\n</code></pre>"},{"location":"use-cases/financial/#fraud-detection","title":"Fraud Detection","text":"<pre><code># Fraud detection model\nfraud_model = load_fraud_model()\n\nbbp_fraud = BlackBoxPrecision(\n    model=fraud_model,\n    explainer_type=ExplanationType.LIME,  # Fast for real-time\n    feature_names=transaction_features\n)\n\n# Real-time transaction analysis\ntransaction = np.array([...])  # Transaction features\n\nresult = bbp_fraud.explain_local(transaction)\n\n# Check if fraud indicators are present\ntop_features = extract_key_features(result, top_k=5)\nfraud_indicators = [\"unusual_location\", \"high_amount\", \"time_of_day\"]\n\nfor feature in top_features[\"features\"]:\n    if feature[\"name\"] in fraud_indicators:\n        if feature[\"importance\"] &gt; threshold:\n            flag_transaction_for_review()\n</code></pre>"},{"location":"use-cases/financial/#regulatory-compliance","title":"Regulatory Compliance","text":"<pre><code>from blackboxpcs.utils import format_explanation_for_audit\nfrom datetime import datetime\nimport json\n\n# Generate explanation for credit decision\nresult = bbp.predict_with_explanation(applicant_data)\n\n# Format for regulatory audit\naudit_record = format_explanation_for_audit(result, include_raw=True)\naudit_record.update({\n    \"timestamp\": datetime.now().isoformat(),\n    \"applicant_id\": \"A12345\",\n    \"decision\": result[\"predictions\"],\n    \"regulatory_compliant\": True\n})\n\n# Save to compliance log\nwith open(\"compliance_log.json\", \"a\") as f:\n    json.dump(audit_record, f)\n    f.write(\"\\n\")\n</code></pre>"},{"location":"use-cases/financial/#impact","title":"Impact","text":"<ul> <li>\u2705 Regulatory Compliance: Meet Fair Lending and other regulations</li> <li>\u2705 Customer Trust: Transparent explanations build confidence</li> <li>\u2705 Bias Detection: Identify and mitigate discriminatory patterns</li> <li>\u2705 Risk Management: Understand fraud detection decisions</li> </ul>"},{"location":"use-cases/financial/#best-practices","title":"Best Practices","text":"<ol> <li>Use Both Explainers: SHAP for compliance, LIME for real-time</li> <li>Monitor for Bias: Regularly audit for protected class discrimination</li> <li>Maintain Audit Trails: Keep all explanations for regulatory review</li> <li>Customer Communication: Provide clear, understandable explanations</li> </ol>"},{"location":"use-cases/financial/#related","title":"Related","text":"<ul> <li>Medical Diagnostics - Regulatory compliance</li> <li>Autonomous Systems - Real-time decisions</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"use-cases/medical/","title":"Medical Diagnostics Use Case","text":"<p>Using Black Box Precision for medical diagnosis with SHAP explanations.</p>"},{"location":"use-cases/medical/#challenge","title":"Challenge","text":"<p>Deploying high-accuracy diagnostic AI without clinical justification is a critical challenge in healthcare. Medical professionals need to understand why a model made a specific diagnosis to:</p> <ul> <li>Build clinical trust</li> <li>Ensure regulatory compliance</li> <li>Maintain audit trails</li> <li>Validate model decisions</li> </ul>"},{"location":"use-cases/medical/#solution","title":"Solution","text":"<p>Black Box Precision with SHAP provides verifiable explanations for every diagnosis, enabling transparent AI-assisted medical decision-making.</p>"},{"location":"use-cases/medical/#implementation","title":"Implementation","text":""},{"location":"use-cases/medical/#setup","title":"Setup","text":"<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom blackboxpcs import BlackBoxPrecision, ExplanationType\n\n# Load medical model (example)\n# In practice, this would be your trained diagnostic model\ndiagnosis_model = load_medical_model()\n\n# Initialize with SHAP for regulatory compliance\nbbp = BlackBoxPrecision(\n    model=diagnosis_model,\n    explainer_type=ExplanationType.SHAP,\n    feature_names=[\n        \"lesion_density\",\n        \"lesion_size\",\n        \"patient_age\",\n        \"family_history\",\n        \"biomarker_1\",\n        \"biomarker_2\"\n    ],\n    class_names=[\"benign\", \"malignant\"]\n)\n</code></pre>"},{"location":"use-cases/medical/#patient-diagnosis-with-explanation","title":"Patient Diagnosis with Explanation","text":"<pre><code># Patient data\npatient_data = np.array([[\n    0.85,  # lesion_density\n    12.0,  # lesion_size (mm)\n    45,    # patient_age\n    1,     # family_history (yes)\n    0.7,   # biomarker_1\n    0.9    # biomarker_2\n]])\n\n# Get prediction with explanation\nresult = bbp.predict_with_explanation(patient_data)\n\n# Extract key features driving the diagnosis\nfrom blackboxpcs.utils import extract_key_features\n\ntop_features = extract_key_features(result, top_k=5, explainer_type=\"shap\")\n\nprint(f\"Diagnosis: {result['predictions']}\")\nprint(\"\\nKey Factors:\")\nfor feature in top_features[\"features\"]:\n    print(f\"  {feature['name']}: {feature['importance']:.4f}\")\n</code></pre>"},{"location":"use-cases/medical/#clinical-audit-trail","title":"Clinical Audit Trail","text":"<pre><code>from blackboxpcs.utils import format_explanation_for_audit\nfrom datetime import datetime\nimport json\n\n# Generate explanation\nresult = bbp.predict_with_explanation(patient_data)\n\n# Format for audit trail\naudit_record = format_explanation_for_audit(result, include_raw=False)\naudit_record.update({\n    \"timestamp\": datetime.now().isoformat(),\n    \"patient_id\": \"P12345\",\n    \"clinician\": \"Dr. Smith\"\n})\n\n# Save to audit log\nwith open(\"clinical_audit_log.json\", \"a\") as f:\n    json.dump(audit_record, f)\n    f.write(\"\\n\")\n</code></pre>"},{"location":"use-cases/medical/#model-validation","title":"Model Validation","text":"<pre><code># Perform comprehensive model audit\naudit_results = bbp.audit_model(\n    X_train,\n    y=y_train,\n    explanation_type=ExplanationType.SHAP\n)\n\nprint(\"Model Accuracy:\", audit_results.get(\"accuracy\"))\nprint(\"\\nFeature Importance Ranking:\")\nfor feature, importance in audit_results[\"explanations\"][\"shap\"][\"feature_importance_ranking\"]:\n    print(f\"  {feature}: {importance:.4f}\")\n\n# Check for bias\nsuspicious_features = [\"patient_age\", \"gender\"]\nfor feature, importance in audit_results[\"explanations\"][\"shap\"][\"feature_importance_ranking\"]:\n    if feature in suspicious_features and importance &gt; threshold:\n        print(f\"\u26a0\ufe0f  Warning: High importance for {feature} - potential bias\")\n</code></pre>"},{"location":"use-cases/medical/#impact","title":"Impact","text":"<ul> <li>\u2705 Clinical Trust: Doctors understand model decisions</li> <li>\u2705 Regulatory Compliance: Audit trails for FDA/regulatory bodies</li> <li>\u2705 Bias Detection: Identify discriminatory features</li> <li>\u2705 Model Validation: Verify model behavior before deployment</li> </ul>"},{"location":"use-cases/medical/#best-practices","title":"Best Practices","text":"<ol> <li>Use SHAP for Medical Applications: Provides mathematical guarantees needed for regulatory compliance</li> <li>Maintain Audit Trails: Log all explanations for compliance</li> <li>Validate Feature Importance: Ensure model uses clinically relevant features</li> <li>Monitor for Bias: Regularly audit model for discriminatory patterns</li> </ol>"},{"location":"use-cases/medical/#related","title":"Related","text":"<ul> <li>Autonomous Systems - Real-time decision validation</li> <li>Financial Systems - Regulatory compliance</li> <li>API Reference - Complete API documentation</li> </ul>"}]}